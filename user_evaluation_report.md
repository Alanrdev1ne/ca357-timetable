# User Evaluation Report

We are Group 36. For our UI project, we chose to create an accessible alternative to the current DCU timetable website. We decided for our implementation of this site we would create timetables for all Computer Applications years.

## "How did you conduct the evaluations (heuristic, user etc.) and why did you choose each?" - Luke Hebblethwaite

For our main method of evaluation, we used an online questionnaire created using Google Forms. We wanted the questionnaire to be short yet effective in gaining insight into the user experience. We decided to use an online questionnaire as our main method of evaluation for a few reasons. Firstly, the members of Group 14 and the members of our group live in different counties. It would be necessary to ask people to travel to Dublin for an interview when the same objective, that is, gaining insight into the user experience, can be achieved through an online questionnaire. Secondly, an online questionnaire built with Google Forms offers many advantages. The results will be displayed in graph and chart form, making the results extremely easy to read and comprehend. Another advantage is that participants can complete the questionnaire at any time of the day at their own leisure. Lastly, this method of evaluation is free and offers fast data collection from participants that are automatically stored for the creators of the Google Form once participants have completed the questionnaire. 

For our second method of user evaluation, we used a heuristic evaluation. We decided to use Jakob Nielsen’s heuristics as they are probably the most-used usability heuristics for user interface design. The main heuristics we felt applied to us were:

Consistency and standards: We wanted the website to look similar to the OpenTimetable website because users are already familiar with the OpenTimetable in different counties and it would be difficult and perhaps unfair to ask people to timetable website. This means that there is no learning curve to become familiar with our website.

Aesthetic and minimalist design: We wanted the website to be as lightweight and minimalistic as possible, hence why we avoided using any Javascript or Javascript frameworks. We made a big effort to avoid any unnecessary or useless information on each webpage.

Help and documentation: We felt that it was important to provide an introduction on the homepage to explain to users what the site was and how to use it.

The reason we choose heuristic evaluation is because it is a detailed, technically sound process that assesses the product against very clear criteria. It also does not require ethical approval and thus can be begun immediately.

## "How many people were part of the evaluation?" - Sean Brereton

Initially, for our evaluation we expected to be questioning the 4 different members of Group 14 that were assigned to our project, using our questionnaire on Google forms. We tried multiple times to get into contact with the group members, but in the end, we did not hear from them. We then decided to ask other members of the course if they could partake in the survey instead. This ended in us having 3 user evaluations. Ideally, we would have liked to have more users opinions, but we felt that 3 was an acceptable amount of users needed to conduct an evaluation with given the circumstances.

## "What questions were you trying to answer with the various methods of evaluation you chose?" - Seán Dyer

There were many questions that we needed to try and answer as a group with the various methods of evaluation that we chose. Since we used both a questionnaire and in-person evaluation, we set out to answer different questions using each of the two methods.

Our questionnaire contained eight questions, asking the users about their experience with our website.

* How likely are you to continue using our website?
* Is there anything we can do to improve this site?
* How responsive do you find our website on mobile?
* What makes this site responsive/not responsive?
* How accessible do you find our website?
* What makes this site accessible/not accessible?
* If you could change one aspect of our site, what would it be and why?
* How likely are you to recommend this site to a friend/colleague?

We deliberately asked direct questions. The reason for this was so that we could get a clear idea of our users thoughts on our website. We asked our users how likely they are to continue using our website and also if they would consider recommending our website to a friend. The reason we asked these questions was so that we could get a feel of what they generally thought about our website and whether they liked it or not.

We also used this questionnaire as a chance to try and find out any places we may have gone wrong. We did this by asking the responders to judge our website based on its accessibility and its responsiveness. We got our responders to judge the website on it's accessibility and responsiveness by rating the website out of 10 on each of those categories. As well as this, we looked for input on anything we could do to improve what we have done, by asking what exactly causes any issues relating to how accessible and responsive the website is, and by asking what the users would change.

The other method of evaluation that we used to answer the questions that needed to be answered was the in-person evaluation. The majority of the participants of this evaluation also completed the questionnaire, but as we planned, the in-person helped us to answer more questions that could not be covered by the questionnaire. By being with the people using our website, we had the chance to garner more detailed information from the users. Since we could see them in their attempts to use our interface, it made it extremely easy for us to find any obvious flaws in our design. There are sometimes things that people do not think of when filling out a questionnaire, but when in the moment, it becomes easier for them to voice any questions or issues that they may have with the website.

## "What did you learn from the evaluation?" - Alan Devine

We learned a lot from this evaluation. Overall the responses the users gave was quite positive, which we were of course very happy to see, as it shows that we’re on the right track. They liked the overall look and feel of the site and found it better than the alternatives, those being OpenTimetable and the older site.

The feedback that we got was very useful, we already have ideas on how we could improve the site to make it more usable, were we to continue working on this website, such as spacing out the entities on the site. That being said, down to the way we asked the questions and issues with contacting the group we didn’t get as much as we could have, which was unfortunate, but more on these later.

Sadly, it wasn’t all positive though, we learned a few difficult lessons during the evaluation process. The first lesson we learned is that relying on others to give feedback is difficult. As stated above it was difficult to make contact with the group assigned to evaluate us, we eventually overcame this with a little help from our friends, who took our survey when they absolutely didn’t have to. I know that this is a somewhat unique circumstance, but it stuck out.

Another thing we learned is that it’s much harder to write good survey questions than expected. It was really a struggle to come up with questions that walked the line between giving us good constructive criticism and stroking our ego. This is an area in which we definitely should have spent more time on as it would have left us with more useful feedback that would have helped tremendously. We think that we left the questions a little too open ended. In hindsight we believe we should have had more concise questions, which would have allowed us to ask more questions, improving the quality of our results.

One last thing that we learned was the importance of in-person evaluation. I can’t stress enough how useful it is to have a face to face with a user. It opens up a whole new range of questions and feedback that are impossible to replicate with a survey. 

Having someone point at the screen and give criticism makes the whole process a lot more tangible.

## "What improvements would you make to the UI having conducted the evaluation?" - Sean Moloney

Having conducted the evaluation, we found a number of areas of our website that need improving as well as some errors and minor graphical issues that take away somewhat from the general user experience.

The most glaring problem with the UI is that the text boxes don’t completely fill the timetable spaces, leaving a bit of white around the outside. It’s nothing major but it would be nicer to look at if it fit properly. One user pointed out that there was a space to the right side of the nav-bar on all pages except CASE1 and the home page.

We would have liked to be able to have a search bar contained within the nav-bar so that the user would be able to search for their specific module and be able to find it easily.  We would also have liked to be able to add a night-mode, or different colour schemes to make it more pleasant for the user to look at and read, but we found that the colour scheme we chose did a good job of being easy to read at a glance and also pleasant to look at for long durations.

We came up with the idea to highlight the current time-slot and day to make it easier and faster to read what room you had to go to, but we came up with the idea quite late in the development process and didn’t have time to implement it so that it worked consistently.

One reviewer pointed out that we should scale the timetable to the size of the user’s screen, they also suggested that we space out each of the days when using mobile view as each of the time slots merges together and if they are of the same type, it can be difficult to differentiate the two.

## "As a group, looking back over the entire assignment, what would you do differently were you to undertake the whole exercise" - Sean Brereton

As a group we learned many things in terms of website prototyping, requirement gathering and user evaluations. Ultimately if we were to do this again the timing of the project is what we all would have done differently. Towards the end we felt quite rushed as the deadline coincided with a lot of other subjects deadlines.  

In terms of requirement gathering, we felt that if we were to do this assignment again that we start off by using more methods of requirement gathering. Although we found the information from our initial requirement gathering survey and our own analysis of other university timetable websites to be very useful, we felt that we could have done with more information. Given the time and resources we would have done focus groups with other university students for more direct feedback and also done earlier prototypes for the website rather than going straight into development.

For our prototype, we all agreed that our biggest problem was our time management and decided that we would have started as soon as we got the assignment. We also felt that it would have been a better idea to have multiple iterations of the website that we could build on. We also would have implemented the previously mentioned UI design improvements to improve the overall look and feel of the website if we had more time.

For our user evaluation, yet again we agreed that time management was our downfall. We decided that we should have attempted to make contact with the group to evaluate our project earlier. This would have allowed us to have more time to wait for their feedback and also more time to write the evaluation report. We also would have added more questions to our user survey to get more in depth feedback to work with.